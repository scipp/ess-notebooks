{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to start\n",
    "\n",
    "Before starting you must:\n",
    "- Have conda installed\n",
    "- `conda env create -f ess-notebooks-latest.yml python=3.7` . The yaml environment file is part of this repository.\n",
    "- fetch the data `git clone git@github.com:scipp/ess-notebooks-data.git` somewhere local \n",
    "- Generate the `dataconfig.py` file using `make_config.py` located in same directory as this notebook. In general, you simply need to point `make_config.py` to the root directory of data you cloned above. Refer to the help `make_config.py --help` for more information. \n",
    "\n",
    "Converted to use scipp and notebook from [this repository](https://github.com/scipp/ess-legacy).\n",
    "\n",
    "For Table of Contents install Jupyter extensions then reload the notebook:\n",
    "`conda install -c conda-forge jupyter_contrib_nbextensions`\n",
    "`jupyter contrib nbextension install --user`\n",
    "`jupyter nbextension enable toc2/main`\n",
    "\n",
    "# Experimental Summary\n",
    "\n",
    "This script has been developed to measure local strain ε defined as ε = ΔL/L0 in a FCC steel sample under elastic strain in a stress rig. Measured at V20, HZB, Berlin, September 2018 by Peter Kadletz.\n",
    "\n",
    "λ = 2dsinθ, where 2θ = π (transmission), edges characterise the Bragg condition and hence λ = 2d. Therefore strain is easily computed from the wavelength measurement of of a Bragg edge directly, using un-loaded vs loaded experimental runs (and reference mesurements). The known Miller indices of the crystal structure (FCC) are used to predict the wavelength where the Bragg edges should exist, which is bound by the reachable wavelength extents for the instrument. This provides an approximate region to apply a fit.  A complement error function is used to fit each Bragg edge, and a refined centre location (λ) for the edge is used in the strain measurement. Because each bragg edge can be identified individually, one can determine anisotropic strain across the unit cell in the reachable crystallographic directions. In addition the image processing allows for spacial grouping so localised effects, such as those on unconstrained edges of the sample or in necking regions of the sample can be treated seperately. The plotted outputs in the script aim to capture this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "# Script setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import scipp\n",
    "except ImportError as e:\n",
    "    print(\"scipp is not available in the PYTHONPATH\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    import mantid\n",
    "except ImportError as e:\n",
    "    print(\"mantid is not available in the PYTHONPATH\")\n",
    "    raise e\n",
    "\n",
    "try:\n",
    "    import dataconfig\n",
    "except ImportError as e:\n",
    "    print(\n",
    "        \"dataconfig is not available. Make sure you have generated it with `make_config.py`.\"\n",
    "    )\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating point precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "float_type = np.float32\n",
    "\n",
    "\n",
    "# Helper to determine scipp dtype from np dtype\n",
    "def scipp_dtype(dtype):\n",
    "    return sc.Variable(value=0, dtype=dtype).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set input and output dirs\n",
    "\n",
    "If your input directory has a different structure this is the cell to modify. \n",
    "Additionally the output directory can be renamed too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets get everything set up\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import scipp as sc\n",
    "import numpy as np\n",
    "\n",
    "import ess.v20.imaging as imaging\n",
    "import ess.v20.imaging.operations as operations\n",
    "import dataconfig\n",
    "from scipy import ndimage, signal\n",
    "\n",
    "local_data_path = os.path.join('ess', 'v20', 'imaging', 'gp2-stress-experiments')\n",
    "data_dir = os.path.join(dataconfig.data_root, local_data_path)\n",
    "output_dir = os.path.join(dataconfig.data_root, 'output')\n",
    "instrument_file = os.path.join(data_dir, 'V20_Definition_GP2.xml')\n",
    "\n",
    "tofs_path = os.path.join(data_dir, 'GP2_Stress_time_values.txt')\n",
    "raw_data_dir = os.path.join(data_dir)\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(\"The following data directory does not exist,\"\n",
    "                            f\" check your make_config.py:\\n{data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = sc.Dataset()\n",
    "sc.compat.mantid.load_component_info(geometry, instrument_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "## Reduction Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Customisable Options:\n",
    "\n",
    "# defining grouping of 2D detector pixels\n",
    "nx_source = 324\n",
    "ny_source = 324\n",
    "grouping_number = 27\n",
    "nx_target = grouping_number\n",
    "ny_target = grouping_number\n",
    "\n",
    "bin_width = (64 * 2.5)  # μS\n",
    "\n",
    "# Rebin regions for each of the 5 WFM frames. Values are in detector time : μS\n",
    "# in the format of [bin-start, bin-end, bin width].\n",
    "# used to crop each image, before stitching them together\n",
    "frame_parameters = [{\n",
    "    \"start\": 15450,\n",
    "    \"stop\": 22942,\n",
    "    \"step\": bin_width\n",
    "}, {\n",
    "    \"start\": 24800,\n",
    "    \"stop\": 32052,\n",
    "    \"step\": bin_width\n",
    "}, {\n",
    "    \"start\": 33791,\n",
    "    \"stop\": 40084,\n",
    "    \"step\": bin_width\n",
    "}, {\n",
    "    \"start\": 41763,\n",
    "    \"stop\": 47457,\n",
    "    \"step\": bin_width\n",
    "}, {\n",
    "    \"start\": 49315,\n",
    "    \"stop\": 54500,\n",
    "    \"step\": bin_width\n",
    "}, {\n",
    "    \"start\": 56500,\n",
    "    \"stop\": 58360,\n",
    "    \"step\": bin_width\n",
    "}]\n",
    "\n",
    "# Used to rebin the summed frame in order to\n",
    "# cut off frames that contain no data\n",
    "rebin_parameters = {\"start\": 8550, \"stop\": 26000, \"step\": bin_width}\n",
    "\n",
    "# Used to shift the cropped frames so that their bins overlap\n",
    "# before summing them together into a single frame\n",
    "frame_shift_increments = [-6630, -2420, -2253, -2095, -1946, -1810]\n",
    "frame_shift_increments = [float(i)\n",
    "                          for i in frame_shift_increments]  # Work around #1114\n",
    "\n",
    "# Pulse references\n",
    "pulse_number_reference = 1.0 / 770956\n",
    "pulse_number_sample = 1.0 / 1280381\n",
    "pulse_number_sample_elastic = 1.0 / 2416839\n",
    "pulse_number_sample_plastic = 1.0 / 2614343\n",
    "\n",
    "# units of transmission, all pixels with transmission higher masking threshold are masked\n",
    "masking_threshold = 0.80\n",
    "\n",
    "# Toggles outputting masked and sliced tiff stacks\n",
    "output_tiff_stack = False\n",
    "\n",
    "# Experiment Metadata\n",
    "measurement_number = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reduction\n",
    "\n",
    "## Load and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# let's get the process started:\n",
    "\n",
    "# Load X values from the TOF file. Though raw contents are NOT TOF corrected yet.\n",
    "ds = sc.Dataset()\n",
    "ds.coords[\"t\"] = sc.Variable([\"t\"],\n",
    "                             unit=sc.units.us,\n",
    "                             values=imaging.read_x_values(tofs_path))\n",
    "ds.coords[\"t\"] *= 1e3\n",
    "\n",
    "\n",
    "def load_and_scale(folder_name, scale_factor):\n",
    "    to_load = os.path.join(raw_data_dir, folder_name)\n",
    "    variable = imaging.tiffs_to_variable(to_load, dtype=float_type)\n",
    "    variable *= scale_factor\n",
    "    return variable\n",
    "\n",
    "\n",
    "ds[\"reference\"] = load_and_scale(folder_name=\"R825-open-beam\",\n",
    "                                 scale_factor=pulse_number_reference)\n",
    "ds[\"sample\"] = load_and_scale(folder_name=\"R825\",\n",
    "                              scale_factor=pulse_number_sample)\n",
    "ds[\"sample_elastic\"] = load_and_scale(folder_name=\"R825-600-Mpa\",\n",
    "                                      scale_factor=pulse_number_sample_elastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "## Add Spectra Coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adds a coordinate for the spectrum\n",
    "ds.coords[\"spectrum\"] = sc.Variable([\"spectrum\"],\n",
    "                                    values=np.arange(ds[\"sample\"].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude-from-export\n",
    "sc.plot.plot(sc.sum(ds['sample'], dim='spectrum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Frame shifts and stitching\n",
    "\n",
    "Pre-calculated frame shifts. This will be replaced with automatic WFM stitching in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO use Neil's wfmess, or similar to avoid hard-coding the edges. This approach is not very nice.\n",
    "frame_shifts = [\n",
    "    sum(frame_shift_increments[:i + 1])\n",
    "    for i in range(len(frame_shift_increments))\n",
    "]\n",
    "\n",
    "\n",
    "def stitch(ds, frame_parameters, frame_shifts, rebin_parameters):\n",
    "    \"\"\"\n",
    "    Stitches the 5 wfm frames data.\n",
    "    It crops out each frame, then shifts it so that all frames align,\n",
    "    and then rebins to the operations bins used for all frames.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "\n",
    "    rebin_params = sc.Variable([\"t\"],\n",
    "                               unit=sc.units.us,\n",
    "                               values=np.arange(**rebin_parameters))\n",
    "\n",
    "    for i, (slice_bins,\n",
    "            shift_parameter) in enumerate(zip(frame_parameters, frame_shifts)):\n",
    "        bins = sc.Variable([\"t\"], values=np.arange(**slice_bins))\n",
    "        # Rebins the whole data to crop it to frame bins\n",
    "        rebinned = sc.rebin(ds, \"t\", bins)\n",
    "        # Shift each frame according to time shift (function of frequency and window opening)\n",
    "        rebinned.coords[\"t\"] += shift_parameter  # Now in TOF\n",
    "        # Rebin to overarching coordinates so that the frame coordinates align\n",
    "        rebinned = sc.rebin(rebinned, \"t\", rebin_params)\n",
    "\n",
    "        frames.append(rebinned)\n",
    "\n",
    "    # All frames now shifted and therefore represent actual TOF. Frames can be summed.\n",
    "    for f in frames[1:]:\n",
    "        frames[0] += f\n",
    "\n",
    "    frames[0].rename_dims({'t': 'tof'})\n",
    "    return frames[0]\n",
    "\n",
    "\n",
    "# Comparing [\"sample\"][\"spectrum\", 37000] data stops matching, as Mantid\n",
    "# and skip appear to handle to partial bin at the end differently. This results\n",
    "# in different counts making data comparison difficult\n",
    "\n",
    "stitched = stitch(ds,\n",
    "                  frame_parameters=frame_parameters,\n",
    "                  frame_shifts=frame_shifts,\n",
    "                  rebin_parameters=rebin_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude-from-export\n",
    "sc.plot.plot(sc.sum(stitched['sample'], dim='spectrum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Masking\n",
    "### Integration step\n",
    "\n",
    "Integrate counts for each spectra by summing over TOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "integrated = sc.sum(stitched, 'tof')\n",
    "\n",
    "# Pull out reference from out dataset to avoid checking for it in loops\n",
    "reference = stitched[\"reference\"].copy()\n",
    "integrated_reference = integrated[\"reference\"].copy()\n",
    "\n",
    "del stitched[\"reference\"]\n",
    "del integrated[\"reference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transmission Masking\n",
    "\n",
    "Divides the integrated sample counts with an open beam reference. Any values > masking threshold\n",
    "will be masked. The adj pixels step checks for random pixels which were left unmasked or masked\n",
    "with all their neighbours having the same mask value. These are forced to True or false depending on\n",
    "their neighbour value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_spectra = stitched.coords[\"spectrum\"].shape[0]\n",
    "bank_width = int(np.sqrt(num_spectra))\n",
    "\n",
    "\n",
    "def mask_non_sample_regions(integrated_dataset, stitched_dataset, var_name):\n",
    "    integrated_spectra = integrated_dataset[var_name]\n",
    "\n",
    "    # This should use sc.greater once #1178 is completed\n",
    "    spectra_masks = np.greater(integrated_spectra.values, masking_threshold)\n",
    "    spectra_masks = sc.Variable([\"spectrum\"], values=spectra_masks)\n",
    "\n",
    "    # Some pixels may be noisy / not strong enough signal so they erranously\n",
    "    # become unmasked / masked from the above operation. This step checks if *all*\n",
    "    # the surrounding pixels are True or False then sets the center pixel to\n",
    "    # that value if they are, removing \"salt and pepper\" noise\n",
    "    spectra_masks = sc.reshape(spectra_masks,\n",
    "                               dims=[\"y\", \"x\"],\n",
    "                               shape=(bank_width, bank_width))\n",
    "\n",
    "    final_mask = operations.mask_from_adj_pixels(spectra_masks)\n",
    "    return sc.Variable([\"spectrum\"], values=final_mask.values.ravel())\n",
    "\n",
    "\n",
    "# Calculate transmission value for all\n",
    "\n",
    "int_norm = integrated / integrated_reference\n",
    "\n",
    "# Store TOF coords back for the export to TIFF stack\n",
    "tof_coords = stitched.coords[\"tof\"]\n",
    "\n",
    "for k in integrated.keys():\n",
    "    print(f\"Masking non-sample regions in {k}\")\n",
    "\n",
    "    p = mask_non_sample_regions(int_norm, stitched, k)\n",
    "    stitched[k].masks['non-sample-region'] = mask_non_sample_regions(\n",
    "        int_norm, stitched, k)\n",
    "\n",
    "    if output_tiff_stack:\n",
    "        print(f\"Exporting tiff stack for {k}\")\n",
    "        tiff_out_dir = os.path.join(output_dir, f\"tiffs_tof_sum_{k}\")\n",
    "        imaging.export_tiff_stack(dataset=int_norm,\n",
    "                                  key=k,\n",
    "                                  base_name=f\"{k}_norm_sum\",\n",
    "                                  output_dir=tiff_out_dir,\n",
    "                                  x_len=nx_source,\n",
    "                                  y_len=ny_source,\n",
    "                                  tof_values=tof_coords.values)\n",
    "        # Print a newline to prevent the saving messages overlapping\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize by open beam\n",
    "normalized = stitched / reference\n",
    "\n",
    "# Replace special values nan and inf\n",
    "replacement = sc.Variable(value=0.0, variance=0.0, dtype=float_type)\n",
    "kwargs = {\"nan\": replacement, \"posinf\": replacement, \"neginf\": replacement}\n",
    "for k in normalized.keys():\n",
    "    sc.nan_to_num(normalized[k].data, out=normalized[k].data, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Output Intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if output_tiff_stack:\n",
    "    tof_values = normalized.coords[\"tof\"].values\n",
    "\n",
    "    imaging.export_tiff_stack(dataset=normalized,\n",
    "                              key=\"sample\",\n",
    "                              x_len=nx_source,\n",
    "                              y_len=ny_source,\n",
    "                              base_name=\"initial_tof\",\n",
    "                              output_dir=os.path.join(output_dir,\n",
    "                                                      \"tiffs_tof_initial\"),\n",
    "                              tof_values=tof_values)\n",
    "    imaging.export_tiff_stack(dataset=normalized,\n",
    "                              key=\"sample_elastic\",\n",
    "                              x_len=nx_source,\n",
    "                              y_len=ny_source,\n",
    "                              base_name=\"elastic_tof\",\n",
    "                              output_dir=os.path.join(output_dir,\n",
    "                                                      \"tiffs_tof_elastic\"),\n",
    "                              tof_values=tof_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convert to wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# Attach instrument geometry\n",
    "normalized = sc.merge(normalized, geometry)\n",
    "wavelength = sc.neutron.convert(normalized, \"tof\", \"wavelength\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filter_func = operations.mean_from_adj_pixels  # Mean filter\n",
    "#filter_func = operations.median_from_adj_pixels # Median filter\n",
    "for k in wavelength.keys():\n",
    "    print(f'{filter_func.__name__} filter over {k}')\n",
    "    yx = sc.reshape(wavelength[k].data,\n",
    "                    dims=[\"wavelength\", \"y\", \"x\"],\n",
    "                    shape=(wavelength[k].shape[0], bank_width, bank_width))\n",
    "    xy_filtered = filter_func(yx)\n",
    "    wavelength[k].data = sc.reshape(xy_filtered,\n",
    "                                    dims=[\"wavelength\", \"spectrum\"],\n",
    "                                    shape=(wavelength[k].shape[0],\n",
    "                                           wavelength[k].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rebin to common bin width\n",
    "\n",
    "As each detector has a different relative position to the sample we\n",
    "need to rebin to a common set of wavelength bins for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wav_x_vals = wavelength[\"sample\"].coords[\"wavelength\"]\n",
    "start_wave = sc.min(wav_x_vals)\n",
    "stop_wave = sc.max(wav_x_vals)\n",
    "\n",
    "widths = wav_x_vals['wavelength', 1:] - wav_x_vals['wavelength', :-1]\n",
    "bin_width = sc.mean(sc.mean(widths, 'wavelength'), 'spectrum')\n",
    "# Arange does not include final bin so overshoot\n",
    "stop_wave += bin_width\n",
    "\n",
    "wavelength_coords = sc.Variable([\"wavelength\"],\n",
    "                                values=np.arange(start_wave.value,\n",
    "                                                 stop_wave.value,\n",
    "                                                 bin_width.value))\n",
    "rebinned_wavelength = sc.rebin(wavelength, \"wavelength\", wavelength_coords)\n",
    "\n",
    "group_size = (float(nx_source) / nx_target) * (float(ny_source) / ny_target)\n",
    "factor = group_size\n",
    "rebinned_wavelength[\"sample\"] *= factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sample - Sum Spectra\n",
    "\n",
    "Performs a weighted sum for all spectra in the sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_sum(da: sc.DataArrayView, dim):\n",
    "\n",
    "    variances = sc.variances(da.data)\n",
    "    data = da / variances\n",
    "    norm = sc.DataArray(data=sc.reciprocal(variances),\n",
    "                        masks=dict(da.masks.items()))\n",
    "    data.masks['zero_error'] = sc.equal(variances,\n",
    "                                        (0.0 * variances.unit).astype(\n",
    "                                            scipp_dtype(float_type)))\n",
    "    norm.masks['zero_error'] = sc.equal(variances,\n",
    "                                        (0.0 * variances.unit).astype(\n",
    "                                            scipp_dtype(float_type)))\n",
    "    return sc.sum(data, dim) / sc.sum(norm, dim)\n",
    "\n",
    "\n",
    "# If we are not interested in strain regions for the unloaded sample, we can just combine all spectra to improve statistics.\n",
    "# maybe another masking before summation, with tougher threshold to exclude boderline pixels.\n",
    "sample_final = weighted_sum(rebinned_wavelength[\"sample\"], \"spectrum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sample elastic - Group detectors\n",
    "\n",
    "Group detectors instead of summing all, since the elastic deformation could\n",
    "happen at any point we want a balance between signal-noise ratio and \n",
    "resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rebinned_wavelength.coords[\"spectrum_mapping\"] = imaging.make_detector_groups(\n",
    "    bank_width, bank_width, nx_target, ny_target)\n",
    "\n",
    "# Group detectors for the sample_elastic / sample_plastic\n",
    "grouped = sc.groupby(rebinned_wavelength, \"spectrum_mapping\").sum(\"spectrum\")\n",
    "grouped_sample_elastic = grouped[\"sample_elastic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-calculate detector positions\n",
    "\n",
    "Pre-calculate average detector positions for plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "histogram_matrices = [sample_final, grouped_sample_elastic]\n",
    "template_histogram = histogram_matrices[1]\n",
    "\n",
    "# Average out detector positions\n",
    "positions_dataset = sc.Dataset()\n",
    "positions_dataset[\"position\"] = rebinned_wavelength.coords[\"position\"].copy()\n",
    "positions_dataset.coords[\"spectrum_mapping\"] = rebinned_wavelength.coords[\n",
    "    \"spectrum_mapping\"]\n",
    "grouped_positions = sc.groupby(positions_dataset,\n",
    "                               \"spectrum_mapping\").mean(\"spectrum\")\n",
    "\n",
    "plots = []\n",
    "for spec_idx in template_histogram.coords[\"spectrum_mapping\"].values:\n",
    "    pos = grouped_positions[\"position\"][\"spectrum_mapping\", int(spec_idx)]\n",
    "    # Append spectrum index and position\n",
    "    plots.append((spec_idx, pos))\n",
    "\n",
    "# We cant access position using (dim, I), since the vector 3d has not dim currently\n",
    "Y_key = 1\n",
    "X_key = 0\n",
    "\n",
    "# We must use round to prevent floating points errors during sorting like -0.05 > 0.05\n",
    "# Sort by y then by x across image plots contain tuples of (spec_index to position), sort by position y, x\n",
    "plots.sort(key=lambda t:\n",
    "           (round(-t[1].value[Y_key], 10), round(t[1].value[X_key], 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fitting\n",
    "## Calculate expected Bragg edge positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Bragg edge position roughly, in Angstrom\n",
    "FCC_a = 3.5  # Angstrom, taken from COD entry 9008469\n",
    "\n",
    "# These miller indices for the given unit cell yield bragg edges between the maximum and minimum wavelength range.\n",
    "#  TODO. should probably calculate which are visible from larger set.\n",
    "indices_FCC = [(1, 1, 1), (2, 0, 0), (2, 2, 0), (3, 1, 1)]\n",
    "\n",
    "\n",
    "def create_Braggedge_list(lattice_constant, miller_indices):\n",
    "    '''\n",
    "    :param miller-indices: like [(1,1,0),(2,0,0),...]\n",
    "    :type miller-indices: list of tuples\n",
    "    '''\n",
    "    coords = [str((h, k, l)) for h, k, l in miller_indices]\n",
    "    interplanar_distances = [\n",
    "        2. * lattice_constant / np.sqrt(h**2 + k**2 + l**2)\n",
    "        for h, k, l in miller_indices\n",
    "    ]\n",
    "\n",
    "    d = sc.DataArray(\n",
    "        sc.Variable(dims=[\"bragg-edge\"],\n",
    "                    values=np.array(interplanar_distances),\n",
    "                    unit=sc.units.angstrom))\n",
    "    d.coords[\"miller-indices\"] = sc.Variable(dims=[\"bragg-edge\"],\n",
    "                                             values=coords)\n",
    "    return d\n",
    "\n",
    "\n",
    "Bragg_edges_FCC = create_Braggedge_list(FCC_a, indices_FCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bragg-edge fitting\n",
    "\n",
    "Fit Bragg-edge peaks for the sample initially. Then take this value to find the position within a window\n",
    "for the elastic sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Bragg_edge_position(xpos_Bragg_edge, x_min_sides, x_max_sides, x_size,\n",
    "                        y_size, plots):\n",
    "    '''Takes limits around a roughly known Bragg edge and fits the Bragg edge to obtain\n",
    "    its exact position. Needs the size (x_size, y_size) and the workspaces containing histogram arrays/matrices.\n",
    "    x_min_side and x_max_side are arbitrary values and are calculated:\n",
    "    bragg_edge_pos * x_min_side or bragg_edge_pos * x_max_side.\n",
    "    '''\n",
    "\n",
    "    fit_list = []\n",
    "    spectrum_list_fitted_sample = []\n",
    "    spectrum_list_fitted_sample_elastic = []\n",
    "    elastic_ws = grouped_sample_elastic\n",
    "    sample_ws = sample_final\n",
    "\n",
    "    # loop for each Bragg edge in the list, find edge positions and save spectrums in lists\n",
    "    for edge_index in range(xpos_Bragg_edge.shape[0]):\n",
    "        fit_list.append([])\n",
    "        spectrum_list_fitted_sample.append([])\n",
    "        spectrum_list_fitted_sample_elastic.append([])\n",
    "\n",
    "        bragg_edge = xpos_Bragg_edge.coords[\"miller-indices\"][\"bragg-edge\",\n",
    "                                                              edge_index]\n",
    "\n",
    "        xpos_guess = xpos_Bragg_edge[\"bragg-edge\", edge_index].value\n",
    "        x_min_fit = xpos_guess - xpos_guess * abs(x_min_sides[edge_index])\n",
    "        x_max_fit = xpos_guess + xpos_guess * abs(x_max_sides[edge_index])\n",
    "\n",
    "        print(\n",
    "            \"Now fitting Bragg edge {} at {:.3f} A (between {:.3f} A and {:.3f} A) across image groups\"\n",
    "            .format(bragg_edge, xpos_guess, x_min_fit, x_max_fit))\n",
    "\n",
    "        # if the full inital sample was taken and no grouping was done\n",
    "        #Fitting the masked sample\n",
    "        params_s, diff_s = sc.compat.mantid.fit(\n",
    "            sample_ws,\n",
    "            mantid_args={\n",
    "                'Function':\n",
    "                f'name=LinearBackground,A0={230},A1={-4};name=UserFunction,Formula=h*erf(a*(x-x0)),h={16},a={-11},x0={xpos_guess}',\n",
    "                'StartX': x_min_fit,\n",
    "                'EndX': x_max_fit\n",
    "            })\n",
    "\n",
    "        v_and_var_s = [\n",
    "            params_s.data['parameter', i]\n",
    "            for i in range(params_s['parameter', :].shape[0])\n",
    "        ]\n",
    "        params = dict(zip(params_s.coords[\"parameter\"].values, v_and_var_s))\n",
    "        d_sample = params[\n",
    "            \"f1.x0\"] / 2.0  # See fit table definition for extract x0\n",
    "\n",
    "        plot_cell_idx = 0\n",
    "        for row in range(y_size):\n",
    "            for col in range(x_size):\n",
    "                spectrum_idx = int(plots[plot_cell_idx][0])\n",
    "\n",
    "                # Fit Bragg edge using values from fit of unstrained sample\n",
    "                elastic_function = f\"name=LinearBackground,A0={params['f0.A0'].value},A1={params['f0.A1'].value};name=UserFunction,Formula=h*erf(a*(x-x0)),h={params['f1.h'].value},a={params['f1.a'].value},x0={params['f1.x0'].value}\"\n",
    "                params_e, diff_e = sc.compat.mantid.fit(\n",
    "                    elastic_ws['spectrum_mapping', spectrum_idx],\n",
    "                    mantid_args={\n",
    "                        'Function': elastic_function,\n",
    "                        'StartX': x_min_fit,\n",
    "                        'EndX': x_max_fit\n",
    "                    })\n",
    "\n",
    "                v_and_var_e = [\n",
    "                    params_e.data['parameter', i]\n",
    "                    for i in range(params_e['parameter', :].shape[0])\n",
    "                ]\n",
    "                elastic_params = dict(\n",
    "                    zip(params_e.coords[\"parameter\"].values, v_and_var_e))\n",
    "                d_sample_elastic = elastic_params[\n",
    "                    \"f1.x0\"] / 2.0  # See fit table definition for extract x0\n",
    "                lattice_strain = d_sample_elastic - d_sample\n",
    "\n",
    "                # define successful fitting\n",
    "                success = (params_s.coords[\"status\"].value == \"success\") and (\n",
    "                    params_e.coords[\"status\"].value == \"success\")\n",
    "\n",
    "                # fitted values STORED in list\n",
    "                fit_list[edge_index].append(\n",
    "                    (spectrum_idx, (row, col), bragg_edge, d_sample,\n",
    "                     d_sample_elastic, lattice_strain, success))\n",
    "\n",
    "                # workspace created from sample fit, workspace index 1 for fitted spectrum (index 0 for data, index 2 for difference curve)\n",
    "                fitted_sample = diff_s['calculated'].values\n",
    "\n",
    "                if params_s.coords[\"status\"].value == \"success\":\n",
    "                    spectrum_list_fitted_sample[edge_index].append(\n",
    "                        (diff_s.coords[\"wavelength\"].values, fitted_sample))\n",
    "                else:\n",
    "                    spectrum_list_fitted_sample[edge_index].append(\n",
    "                        (diff_s.coords[\"wavelength\"].values,\n",
    "                         np.zeros_like(fitted_sample)))\n",
    "\n",
    "                fitted_elastic = diff_e['calculated'].values\n",
    "                # workspace created from fit of sample under elastic deformation\n",
    "                if params_e.coords[\"status\"].value == \"success\":\n",
    "                    spectrum_list_fitted_sample_elastic[edge_index].append(\n",
    "                        (diff_e.coords[\"wavelength\"].values, fitted_elastic))\n",
    "                else:\n",
    "                    spectrum_list_fitted_sample_elastic[edge_index].append(\n",
    "                        (diff_e.coords[\"wavelength\"].values,\n",
    "                         np.zeros_like(fitted_elastic)))\n",
    "                plot_cell_idx += 1\n",
    "\n",
    "    return fit_list, spectrum_list_fitted_sample, spectrum_list_fitted_sample_elastic\n",
    "\n",
    "fit_list, spectrum_list_fitted_sample, spectrum_list_fitted_sample_elastic = \\\n",
    "    Bragg_edge_position(xpos_Bragg_edge=Bragg_edges_FCC,\n",
    "                                      x_min_sides=[0.05, 0.1, 0.1, 0.05],#[0.1, 0.1, 0.1, 0.05]\n",
    "                                      x_max_sides=[0.1, 0.05, 0.1, 0.1],#[0.1, 0.1, 0.1, 0.1]\n",
    "                                      x_size=nx_target,\n",
    "                                      y_size=ny_target,\n",
    "                                      plots=plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate single tile plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# exclude-from-export\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "for i in plt.get_fignums():\n",
    "    plt.close(i)\n",
    "\n",
    "\n",
    "def tileplot_colorcode(fit_list,\n",
    "                       nx_target,\n",
    "                       ny_target,\n",
    "                       outlier_threshold,\n",
    "                       output_filename_stem='tileplot_color'):\n",
    "    # for each part of the fit list containing the fit values of each Bragg edge, create a color plot.\n",
    "    output_filename_list = []\n",
    "    for fit_be_list in fit_list:\n",
    "        bragg_edge = fit_be_list[0][\n",
    "            2].values  # the calculated Bragg edge TOF position\n",
    "        d_spacing = fit_be_list[0][3]  # the calculated Bragg edge TOF position\n",
    "        output_filename = '{}_{:.3f}A_{}'.format(output_filename_stem,\n",
    "                                                 d_spacing.value, bragg_edge)\n",
    "        # Make a 2D image of the strain values\n",
    "        print(('Plotting color-coded tile plot of Bragg edge {}.'.format(\n",
    "            bragg_edge)))\n",
    "        fig, ax = plt.subplots()\n",
    "        strains = np.zeros([ny_target, nx_target])\n",
    "        plots_counter = 0\n",
    "        for row in range(ny_target):\n",
    "            for col in range(nx_target):\n",
    "                if (fit_be_list[plots_counter][-1] and outlier_threshold[0] <\n",
    "                        fit_be_list[plots_counter][-2].value <\n",
    "                        outlier_threshold[1]):\n",
    "                    strains[row, col] = fit_be_list[plots_counter][-2].value\n",
    "                plots_counter += 1\n",
    "        im = ax.imshow(strains,\n",
    "                       origin=\"upper\",\n",
    "                       norm=colors.SymLogNorm(linthresh=5.0e-3, vmin=-5.0e-3, vmax=5.0e-3, base=np.e),\n",
    "                       cmap=\"RdBu\")\n",
    "        cb = plt.colorbar(im)\n",
    "        cb.set_label(\"${}$ Lattice strain $\\\\varepsilon$\".format(bragg_edge))\n",
    "        output_filename_list.append(output_filename)\n",
    "    return output_filename_list\n",
    "\n",
    "\n",
    "# i.e. -1e-2, 1e-2 means only -1% to 1% values of lattice strain are shown\n",
    "outlier_threshold_color_plot = (-5e-2, 5e-2)\n",
    "\n",
    "output_filename_tileplot_color = '{:03d}_tileplot_color_{}xy_{}usbin_{}thresh_{:.3f}-{:.3f}plotthresh_initintegr'.format(\n",
    "    measurement_number, grouping_number, bin_width, masking_threshold,\n",
    "    outlier_threshold_color_plot[0], outlier_threshold_color_plot[1])\n",
    "fignames_col = tileplot_colorcode(\n",
    "    fit_list=fit_list,\n",
    "    nx_target=nx_target,\n",
    "    ny_target=ny_target,\n",
    "    outlier_threshold=outlier_threshold_color_plot,\n",
    "    output_filename_stem=os.path.join(output_dir,\n",
    "                                      output_filename_tileplot_color))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
