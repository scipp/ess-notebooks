{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grand-coupon",
   "metadata": {},
   "source": [
    "# Data reduction for Amor using `scipp`\n",
    "\n",
    "In this notebook, we will look in detail at the reduction of data collected from the\n",
    "[Amor](https://www.psi.ch/en/sinq/amor) using [scipp](https://scipp.github.io).\n",
    "This notebook aims to communicate how data reduction is performed transparently and understandably.\n",
    "This is a living document and there are plans to update this as necessary with changes in the data reduction methodology and code.\n",
    "\n",
    "All of the steps given here are implemented in the\n",
    "[ReflData](https://scipp.github.io/ess/techniques/reflectometry/data.html#ess.reflectometry.data.ReflData),\n",
    "[AmorData](https://scipp.github.io/ess/instruments/amor/amor_data.html#ess.amor.amor_data.AmorData),\n",
    "[AmorReference](https://scipp.github.io/ess/instruments/amor/amor_data.html#ess.amor.amor_data.AmorReference),\n",
    "and\n",
    "[Normalisation](https://scipp.github.io/ess/instruments/amor/amor_data.html#ess.amor.amor_data.Normalisation)\n",
    "classes that are available in the [ess](https://scipp.github.io/ess/index.html) package.\n",
    "Furthermore, is it shown how the functionality detailed here can be easily accessed through these classes in this\n",
    "[simple reduction notebook](https://scipp.github.io/ess-notebooks/reflectometry/amor_reduction.html).\n",
    "\n",
    "We will begin by importing the modules that are necessary for this notebook and loading the data.\n",
    "The [sample.nxs](https://github.com/scipp/ess-notebooks-data/raw/main/ess/amor/sample.nxs) file is the experimental data file of interest and\n",
    "[reference.nxs](https://github.com/scipp/ess-notebooks-data/raw/main/ess/amor/references.nxs)\n",
    "is the reference measurement of the neutron supermirror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dataconfig\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "import scipp as sc\n",
    "import scippneutron as scn\n",
    "from ess.reflectometry.data import ReflData\n",
    "from ess.reflectometry import HDM, G_ACC\n",
    "from ess.amor.amor_data import AmorData\n",
    "\n",
    "local_data_path = os.path.join('ess-notebooks', 'amor')\n",
    "data_dir = os.path.join(dataconfig.data_root, local_data_path)\n",
    "data_file = os.path.join(data_dir, 'sample.nxs')\n",
    "reference_file = os.path.join(data_dir, 'reference.nxs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-mexican",
   "metadata": {},
   "source": [
    "## Raw data parsing\n",
    "\n",
    "The data collected at the Amor instrument is written to a [NeXus](https://www.nexusformat.org)\n",
    "after the data collection is complete.\n",
    "This NeXus file can be read into `scipp` with the `scn.load_nexus` function,\n",
    "once read we will change the coordinate system to match that commonly used in reflectometry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scn.load_nexus(data_file)\n",
    "x = data.coords['position'].fields.x.copy()\n",
    "data.coords['position'].fields.x = data.coords['position'].fields.z\n",
    "data.coords['position'].fields.z = data.coords['position'].fields.y\n",
    "data.coords['position'].fields.y = x\n",
    "# Amor records event time-of-flight in nanoseconds,\n",
    "# but for convenience we convert this to the more commonly used unit of microseconds.\n",
    "data.bins.coords['tof'] = sc.to_unit(data.bins.coords['tof'].astype('float64'), 'us')\n",
    "data.coords['tof'] = sc.to_unit(data.coords['tof'].astype('float64'), 'us')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-puppy",
   "metadata": {},
   "source": [
    "It is possible to visualise this data as a `scipp.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-jesus",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-object",
   "metadata": {},
   "source": [
    "The data is stored in a binned structure,\n",
    "where each bin is a pixel on the detector containing each neutron event that was measured there.\n",
    "\n",
    "We can inspect the contents of the data by also plotting the spectrum for each detector pixel as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-kelly",
   "metadata": {},
   "source": [
    "We note that two pulses are present in this dataset, which we will need to fold.\n",
    "\n",
    "It is possible to visualise the detector, with the neutron events shown in the relevant pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "scn.instrument_view(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-england",
   "metadata": {},
   "source": [
    "## Time-of-flight correction and pulse folding\n",
    "\n",
    "The time-of-flight values present in the `data` above are taken from the `event_time_offset`, $t_{\\text{eto}}$.\n",
    "Therefore, it is necessary to account for an offset due to the chopper pulse generation at the Amor instrument.\n",
    "This correction is performed by considering half of the reciprocal rotational velocity of the chopper, $\\tau$,\n",
    "and the phase offset between the chopper pulse and the time-of-flight $0$, $\\phi_{\\text{chopper}}$, \n",
    "\n",
    "$$\n",
    "t_{\\text{offset}} = \\tau \\phi_{\\text{chopper}}.\n",
    "$$\n",
    "\n",
    "At the same time, we wish to fold the two pulses of length $\\tau$.\n",
    "So we effectively have two offsets to apply to the time coordinate:\n",
    "\n",
    "1. $t_{\\text{offset}}$ should be subtracted from the time coordinate in the range $[0, \\tau]$\n",
    "1. $t_{\\text{offset}} + \\tau $ should be subtracted from the time coordinate in the range $[\\tau, 2\\tau]$\n",
    "\n",
    "<!-- The time-of-flight is then found as, \n",
    "\n",
    "$$\n",
    "t = [(t_{\\text{eto}} + t_{\\text{offset}} - t_{\\text{cut}} + \\tau) \\;\\text{mod}\\; \\tau] + t_{\\text{cut}},\n",
    "$$\n",
    "\n",
    "where, $\\text{mod}$ is the remainder between the two values and $t_{\\text{cut}}$ is found from the minimum wavelength, $\\lambda$, to be used.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-procurement",
   "metadata": {},
   "source": [
    "First, we compute $\\tau$ and $t_{\\text{offset}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "chopper_speed = 20 / 3 * 1e-6 / sc.units.us\n",
    "tau = 1 / (2 * chopper_speed)\n",
    "chopper_phase = -8.0 * sc.units.deg\n",
    "t_offset = tau * chopper_phase / (180.0 * sc.units.deg)\n",
    "sc.to_html(tau)\n",
    "sc.to_html(t_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-japan",
   "metadata": {},
   "source": [
    "We now make two bins in the data along the time of flight dimension, one for each pulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = sc.array(dims=['tof'], values=[0., tau.value, 2*tau.value], unit=tau.unit)\n",
    "data = sc.bin(data, edges=[edges])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-funds",
   "metadata": {},
   "source": [
    "Next, we construct a variable containing the offset for each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = sc.concatenate(t_offset, t_offset - tau, 'tof')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-thriller",
   "metadata": {},
   "source": [
    "Finally, we apply the offsets on both bins,\n",
    "and apply new bin boundaries to exclude the (now empty) second pulse range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.bins.coords['tof'] += offset\n",
    "data = sc.bin(data, edges=[sc.concatenate(0.*sc.units.us, tau, 'tof')])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-exhaust",
   "metadata": {},
   "source": [
    "Having corrected the time-of-flight values,\n",
    "we visualize the time-of-flight spectrum of all pixels summed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d488f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.bins.concatenate('detector_id').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-command",
   "metadata": {},
   "source": [
    "## Time-of-flight to wavelength conversion\n",
    "\n",
    "The time-of-flight has been determined, therefore it is now possible to convert from time-of-flight to wavelength, an important step in determining $q_z$. \n",
    "`scipp` includes built-in functionality to perform this conversion based on the instrument geometry. \n",
    "However, the `source_position` must be modified to account for the fact that the source is defined by the chopper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.attrs[\"source_position\"] = sc.vector(\n",
    "    value=[-15., 0., 0.], \n",
    "    unit=sc.units.m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-defensive",
   "metadata": {},
   "source": [
    "The `scn.convert` function makes use of the definition of the sample position at $[0, 0, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.coords['sample_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scn.convert(data, origin='tof', target='wavelength', scatter=True)\n",
    "# Select desired wavelength range\n",
    "wavelength_range = sc.array(dims=['wavelength'], values=[2.4, 15.], unit='angstrom')\n",
    "data = sc.bin(data, edges=[wavelength_range])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-sweden",
   "metadata": {},
   "source": [
    "In the above operation, the `'tof'` coordinate is lost, this is not the case for the `ReflData` or `AmorData` objects. \n",
    "\n",
    "Similar to the time-of-flight, it is possible to visualise the $\\lambda$-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.bins.concatenate('detector_id').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-password",
   "metadata": {},
   "source": [
    "## Theta calculation and gravity correction\n",
    "\n",
    "The Amor reflectometer scatters neutrons vertically in space from a horizontal scattering surface. \n",
    "Therefore, when the scattering angle, $\\theta$, is found,\n",
    "it is necessary to consider the effect that gravity will have on the neutron's trajectory. \n",
    "\n",
    "<center>\n",
    "    <img src='gravity.png' width=60%>\n",
    "    <i>\n",
    "        The effect of gravity for a neutron with a velocity of 20 ms<sup>-1</sup>; the solid blue line shows the trajectory of a neutron reflected from a surface under the influence of gravity, the dashed green line shows the trajectory without gravity if no correction, the solid orange line shows the true trajectory if the reflection were to occur with no gravity present.\n",
    "    </i>\n",
    "</center>\n",
    "\n",
    "The figure above shows the trajectory of a neutron reflecting from a sample and being detected on the detector (the blue line). \n",
    "Initially assuming that all of the neutrons are incident at the point $(0, 0)$ (the influence of beam and sample width is [considered below](#Resolution-functions)). \n",
    "During the trajectory, the neutron is acted on by the force of gravity, leading to the parabolic motion shown. \n",
    "It is clear that if $\\theta$ were calculated without accounting for gravity (dashed green line), then the angle would be underestimated. \n",
    "\n",
    "The trajectory for the detected neutron can be found by considering the kinematic equations of motion. \n",
    "The time taken for the neutron to travel from $x=x_0$ to $x=x_d$ is, \n",
    "\n",
    "$$\n",
    "t_d = \\frac{x_d - x_0}{v_x}, \n",
    "$$\n",
    "\n",
    "where, $v_x$ is the velocity of the neutron in the x-dimension. \n",
    "It is assumed that the velocity in this dimension is the main component of the total velocity and that this doesn't change over time. \n",
    "Therefore, we can calculate $v_x$ from the neutron's wavelength as, \n",
    "\n",
    "$$\n",
    "v_x = \\frac{h}{m_n\\lambda}.\n",
    "$$\n",
    "\n",
    "This can be found as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events.coords[\"velocity\"] = sc.to_unit(HDM / data.events.coords['wavelength'], 'm/s')\n",
    "data.events.coords[\"velocity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-summer",
   "metadata": {},
   "source": [
    "It is assumed that the parabolic motion does not reach a maximum before the neutron is incident on the detector. \n",
    "Therefore, the velocity in the $y$-dimension at the time of reflection can be found, \n",
    "\n",
    "$$\n",
    "v_z(0) = \\frac{z_d - z_0 - 0.5 a t_d^2}{t_d},\n",
    "$$\n",
    "\n",
    "where $z_0$ is the initial position of the neutron, $z_0 = 0$, and $a$ is the acceleration due to gravity, $a = -g = 9.80665\\;\\text{ms}^{-2}$. \n",
    "Using this, the $z$-position of the neutron can be found at any time, $t$, \n",
    "\n",
    "$$\n",
    "z(t) = z_0 + v_z(0)t + 0.5 a t^2, \n",
    "$$\n",
    "\n",
    "or any $x$-position (with $v_z(0)$ expanded), \n",
    "\n",
    "$$\n",
    "z(x) = z_0 + \\Bigg[(x - x_0) \\bigg(-\\frac{a (x_d - x_0)^2}{2v_z^2} - z_0 + z_d\\bigg)\\Bigg]\\frac{1}{x_d - x_0} + \\frac{a (x - x_0)^2}{2v_x^2}.\n",
    "$$\n",
    "\n",
    "The derivative of this with respect to $x$ can then be found, \n",
    "\n",
    "$$\n",
    "z'(x) = \\bigg(\\frac{-a(x_d-x_0)^2}{2v_x^2}-z_0 + z_d\\bigg)\\frac{1}{x_d-x_0} + \\frac{a(x-x_0)}{v_x^2}, \n",
    "$$ \n",
    "\n",
    "This can be simplified when $x=x_0$ to, \n",
    "\n",
    "$$\n",
    "z'(x_0) = \\frac{-a (x_d-x_0)}{2v_x^2} + \\frac{z_d-z_0}{x_d-x_0}.\n",
    "$$\n",
    "\n",
    "From which $\\theta$ can be found,\n",
    "\n",
    "$$\n",
    "\\theta = -\\omega + \\tan^{-1}\\bigg(\\frac{z'(x_0)x_d + z_0 - z'(x_0)x_0}{x_d}\\bigg),\n",
    "$$\n",
    "\n",
    "where, $\\omega$ is the angle of the sample relative to the laboratory horizon. \n",
    "\n",
    "We can show this in action, first by defining a function for the derivative above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_dash0(velocity, x_origin, z_origin, x_measured, z_measured):\n",
    "    \"\"\"\n",
    "    Evaluation of the first dervative of the kinematic equations \n",
    "    for for the trajectory of a neutron reflected from a surface.\n",
    "    \n",
    "    :param velocity: Neutron velocity\n",
    "    :type velocity: scipp._scipp.core.VariableView\n",
    "    :param x_origin: The z-origin position for the reflected neutron\n",
    "    :type x_origin: scipp._scipp.core.Variable\n",
    "    :param z_origin: The y-origin position for the reflected neutron\n",
    "    :type z_origin: scipp._scipp.core.Variable \n",
    "    :param x_measured: The z-measured position for the reflected neutron\n",
    "    :type x_measured: scipp._scipp.core.Variable\n",
    "    :param z_measured: The y-measured position for the reflected neutron\n",
    "    :type z_measured: scipp._scipp.core.Variable\n",
    "    \n",
    "    :return: The gradient of the trajectory of the neutron at the origin position.\n",
    "    :rtype: scipp._scipp.core.VariableView\n",
    "    \"\"\"\n",
    "    velocity2 = velocity * velocity\n",
    "    x_diff = x_measured - x_origin\n",
    "    z_diff = z_measured - z_origin\n",
    "    return -0.5 * sc.norm(G_ACC) * x_diff / velocity2 + z_diff / x_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-hearts",
   "metadata": {},
   "source": [
    "The angle is found by evaluating the position of each pixel with respect to the sample position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_measured = data.coords['position'].fields.z\n",
    "x_measured = data.coords['position'].fields.x\n",
    "z_origin = data.coords['sample_position'].fields.z\n",
    "x_origin = data.coords['sample_position'].fields.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-raise",
   "metadata": {},
   "source": [
    "The gradient and hence the angle can then be found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dash = z_dash0(\n",
    "    data.bins.coords[\"velocity\"],\n",
    "    x_origin,\n",
    "    z_origin,\n",
    "    x_measured,\n",
    "    z_measured)\n",
    "intercept = z_origin - z_dash * x_origin\n",
    "z_true = x_measured * z_dash + intercept\n",
    "angle = sc.to_unit(\n",
    "    sc.atan(z_true / x_measured).bins.constituents[\"data\"], \n",
    "    'deg')\n",
    "angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-forty",
   "metadata": {},
   "source": [
    "The value of $\\theta$ can then be found by accounting for the sample angle, $\\omega$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 0.0 * sc.units.deg\n",
    "data.events.coords['theta'] = -omega + angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-evening",
   "metadata": {},
   "source": [
    "This can be visualised like the wavelength and time-of-flight data earlier, or as a two-dimensional histogram of the intensity as a function of $\\lambda$/$\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_t = sc.max(data.events.coords['theta'])\n",
    "min_t = sc.min(data.events.coords['theta'])\n",
    "bins_t = sc.linspace(\n",
    "    dim='theta',\n",
    "    start=min_t.value,\n",
    "    stop=max_t.value,\n",
    "    num=50,\n",
    "    unit=data.events.coords['theta'].unit)\n",
    "max_w = sc.max(wavelength_range)\n",
    "min_w = sc.min(wavelength_range)\n",
    "bins = sc.linspace(\n",
    "    dim='wavelength',\n",
    "    start=min_w.value,\n",
    "    stop=max_w.value,\n",
    "    num=100,\n",
    "    unit=data.events.coords['wavelength'].unit)\n",
    "sc.bin(data.events, edges=[bins_t, bins]).bins.sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-determination",
   "metadata": {},
   "source": [
    "## Determination of $q_z$-vector\n",
    "\n",
    "The $\\lambda$ and $\\theta$ can be brought together to calculate the the wavevector, $q_z$, \n",
    "\n",
    "$$\n",
    "q_z = \\frac{4\\pi \\sin{\\theta}}{\\lambda}.\n",
    "$$\n",
    "\n",
    "This is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events.coords['qz'] = 4. * np.pi * sc.sin(\n",
    "    data.events.coords['theta']) / data.events.coords['wavelength']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-bikini",
   "metadata": {},
   "source": [
    "We can then investigate the intensity as a function of $q_z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q = 0.08 * sc.Unit('1/angstrom')\n",
    "min_q = 0.008 * sc.Unit('1/angstrom')\n",
    "bins_q = sc.Variable(\n",
    "    values=np.linspace(min_q.value, max_q.value, 200), \n",
    "    unit=data.events.coords['qz'].unit,\n",
    "    dims=['qz'])\n",
    "sc.histogram(data.events, bins=bins_q).plot(norm='log')\n",
    "bins.concatenate('detector_id').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-disposal",
   "metadata": {},
   "source": [
    "## Illumination correction\n",
    "\n",
    "The above intensity as a function of $q_z$ fails to account for the difference in illumination as a function of $\\theta$, due to the size of the beam which illuminates the sample decreases with increasing $\\theta$. \n",
    "\n",
    "<center>\n",
    "    <img src='beam_size.png' width=60%>\n",
    "    <i>\n",
    "        The effect of $\\theta$ on the beam size on the sample (blue line) and the resulting scale factor where the size of the sample is considered (orange line); where the beam size is 2 mm and the sample is 10 mm.\n",
    "    </i>\n",
    "</center>\n",
    "\n",
    "Using the assumption that the beam size describes the full width at half maximum of the beam, the following function can be used to determine the scale factor necessary to account for the illumination variation as a function of $\\theta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-stocks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination_correction(beam_size, sample_size, theta):\n",
    "    \"\"\"\n",
    "    The factor by which the intensity should be multiplied to account for the\n",
    "    scattering geometry, where the beam is Gaussian in shape.\n",
    "\n",
    "    :param: beam_size: Width of incident beam\n",
    "    :type beam_size: scipp._scipp.core.Variable\n",
    "    :param sample_size: Width of sample in the dimension of the beam\n",
    "    :type sample_size: scipp._scipp.core.Variable\n",
    "    :param theta: Incident angle\n",
    "    :type theta: scipp._scipp.core.Variable\n",
    "    \n",
    "    :return: Correction factor\n",
    "    :rtype: scipp._scipp.core.Variable\n",
    "    \"\"\"\n",
    "    beam_on_sample = beam_size / sc.sin(theta)\n",
    "    fwhm_to_std = 2 * np.sqrt(2 * np.log(2))\n",
    "    scale_factor = erf(\n",
    "        (sample_size / beam_on_sample * fwhm_to_std).values)\n",
    "    return sc.Variable(values=scale_factor, dims=theta.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-status",
   "metadata": {},
   "source": [
    "This illumination correction scale factor is applied to each event as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events.data /= illumination_correction(\n",
    "    2 * sc.Unit('mm'), \n",
    "    10 * sc.Unit('mm'), \n",
    "    data.events.coords['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-finland",
   "metadata": {},
   "source": [
    "We can then show the reflected intensity as a function of a linear $q_z$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_q = 0.08 * sc.Unit('1/angstrom')\n",
    "min_q = 0.008 * sc.Unit('1/angstrom')\n",
    "bins_q = sc.linspace(\n",
    "    dim='qz',\n",
    "    start=min_q.value, \n",
    "    stop=max_q.value,\n",
    "    num=200,\n",
    "    unit=data.events.coords['qz'].unit)\n",
    "sc.histogram(data.events, bins=bins_q).plot(norm='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-korean",
   "metadata": {},
   "source": [
    "# Resolution functions\n",
    "\n",
    "We will consider three contributions to the resolution function at the Amor instrument:\n",
    "\n",
    "- $\\sigma \\lambda/\\lambda$: this is due to the double-blind chopper and depends on the distance between the two choppers, $d_{CC}$, and the distance from the halfway between the two choppers to the detector $d_{CD}$, \n",
    "\n",
    "$$\n",
    "\\frac{\\sigma \\lambda}{\\lambda} = \\frac{d_{CC}}{2d_{CD}\\sqrt{2\\ln2}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "chopper_chopper_distance=0.49 * sc.units.m\n",
    "data.attrs[\"sigma_lambda_by_lambda\"] = chopper_chopper_distance / (\n",
    "    data.coords[\"position\"].fields.z - data.coords[\"source_position\"].fields.z)\n",
    "data.attrs[\"sigma_lambda_by_lambda\"] /= 2 * np.sqrt(2 * np.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-trance",
   "metadata": {},
   "source": [
    "- $\\sigma \\gamma/\\theta$: this is to account for the spatial resolution of the detector pixels, which have a FWHM of $\\Delta z \\approx 0.5\\;\\text{mm}$ and the sample to detector distance, $d_{SD}$, \n",
    "\n",
    "$$\n",
    "\\frac{\\sigma \\gamma}{\\theta} = \\frac{1}{2\\theta\\sqrt{2\\ln2}}\\arctan{\\frac{\\Delta z}{d_{SD}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_resolution = 0.0005 * sc.units.m\n",
    "fwhm = sc.to_unit(\n",
    "    sc.atan(\n",
    "        spatial_resolution / (\n",
    "                data.coords[\"position\"].fields.z - \n",
    "                data.coords[\"source_position\"].fields.z)),\"deg\",)\n",
    "sigma_gamma = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
    "data.attrs[\"sigma_gamma_by_theta\"] = sigma_gamma / data.bins.coords['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-schema",
   "metadata": {},
   "source": [
    "- $\\sigma \\theta/\\theta$: finally, this accounts for the width of the beam on the sample or the size of the sample (which ever is smaller), the FWHM is the range of possible $\\theta$-values, accounting for the gravity correction discussed above and the sample/beam geometry,\n",
    "\n",
    "$$\n",
    "\\frac{\\sigma\\theta}{\\theta} = \\frac{1}{2\\sqrt{2\\ln2}}\\frac{\\theta_{\\text{max}} - \\theta_{\\text{min}}}{\\theta_{\\text{mid}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 0.001 * sc.units.m\n",
    "sample_size = 0.5 * sc.units.m\n",
    "beam_on_sample = beam_size / sc.sin(data.bins.coords['theta'])\n",
    "half_beam_on_sample = (beam_on_sample / 2.0)\n",
    "offset_positive = data.coords['sample_position'].copy()\n",
    "offset_positive.fields.x + half_beam_on_sample\n",
    "offset_negative = data.coords['sample_position'].copy()\n",
    "offset_negative.fields.x - half_beam_on_sample\n",
    "z_measured = data.coords[\"position\"].fields.z\n",
    "x_measured = data.coords[\"position\"].fields.x\n",
    "x_origin = offset_positive.fields.x\n",
    "z_origin = offset_positive.fields.z\n",
    "z_dash = z_dash0(data.bins.coords[\"velocity\"], x_origin, z_origin, x_measured, z_measured)\n",
    "intercept = z_origin - z_dash * x_origin\n",
    "z_true = x_measured * z_dash + intercept\n",
    "angle_max = sc.to_unit(sc.atan(z_true / x_measured), 'deg')\n",
    "x_origin = offset_negative.fields.x\n",
    "z_origin = offset_negative.fields.z\n",
    "z_dash = z_dash0(data.bins.coords[\"velocity\"], x_origin, z_origin, x_measured, z_measured)\n",
    "intercept = z_origin - z_dash * x_origin\n",
    "z_true = x_measured * z_dash + intercept\n",
    "angle_min = sc.to_unit(sc.atan(z_true / x_measured), 'deg')\n",
    "fwhm_to_std = 2 * np.sqrt(2 * np.log(2))\n",
    "sigma_theta_position = (angle_max - angle_min) / fwhm_to_std\n",
    "data.attrs[\"sigma_theta_by_theta\"] = sigma_theta_position / data.bins.coords['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-moral",
   "metadata": {},
   "source": [
    "These three contributors to the resolution function are combined to give the total resolution function, \n",
    "\n",
    "$$\n",
    "\\frac{\\sigma q_z}{q_z} = \\sqrt{\\bigg(\\frac{\\sigma\\lambda}{\\lambda}\\bigg)^2+\\bigg(\\frac{\\sigma\\gamma}{\\theta}\\bigg)^2+\\bigg(\\frac{\\sigma\\theta}{\\theta}\\bigg)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.events.attrs['sigma_q_by_q'] = sc.sqrt(\n",
    "    data.attrs['sigma_lambda_by_lambda'] * data.attrs['sigma_lambda_by_lambda']\n",
    "    + data.attrs['sigma_gamma_by_theta'] * data.attrs['sigma_gamma_by_theta']\n",
    "    + data.attrs[\"sigma_theta_by_theta\"] * data.attrs[\"sigma_theta_by_theta\"]).bins.constituents['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-world",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "The above steps are performed both on the sample of interest and a reference supermirror, using the `AmorData` class, thus enabling normalisation to be achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-protection",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample = AmorData(data_file)\n",
    "reference = AmorData(reference_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-shepherd",
   "metadata": {},
   "source": [
    "These result in the plots shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.histogram(sample.data.events, bins=bins_q).plot(norm='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.histogram(reference.data.events, bins=bins_q).plot(norm='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-method",
   "metadata": {},
   "source": [
    "The specification of the supermirror defines the normalisation that is used for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_value = 5\n",
    "supermirror_critical_edge = (0.022) * sc.Unit('1/angstrom')\n",
    "supermirror_max_q = m_value * supermirror_critical_edge\n",
    "supermirror_alpha = 0.25 / 0.088 * sc.units.angstrom\n",
    "normalisation = sc.ones(\n",
    "    dims=['event'], \n",
    "    shape=reference.data.events.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-client",
   "metadata": {},
   "source": [
    "The data array is first masked at values greater than the upper limit of the supermirror. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.data.events.masks['normalisation'] = reference.data.events.coords['qz'] >= supermirror_max_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-sweden",
   "metadata": {},
   "source": [
    "The value of the $q$-dependent normalisation, $n(q)$, is then defined as such that $n(q)=1$ for values of $q$ less then the critical edge of the supermirror and for values between this and the supermirror maximum the normalisation is, \n",
    "\n",
    "$$ n(q) = \\frac{1}{1 - \\alpha(q - c_{\\text{sm}})}. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = (reference.data.events.coords['qz'] < supermirror_critical_edge).astype(sc.dtype.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq = 1.0 / (1.0 - supermirror_alpha * (reference.data.events.coords['qz'] - supermirror_critical_edge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation = lim + (1 - lim) * nq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-muscle",
   "metadata": {},
   "source": [
    "Applying this normalisation to the reference measurement data will return the neutron intensity as a function of $q$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference.data.bins.constituents['data'].data = reference.data.events.data / normalisation.astype(sc.dtype.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.histogram(reference.data.events, bins=bins_q).plot(norm='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-grade",
   "metadata": {},
   "source": [
    "Binning this normalised description on the neutron intensity and the sample measurement in the same $q$-values allows the normalisation to be applied to the sample (note that scaling between these to measurements to account for counting time may be necessary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_sample = sc.histogram(sample.data.events, bins=bins_q)\n",
    "binned_reference = sc.histogram(reference.data.events, bins=bins_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_sample = binned_sample / binned_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_sample.plot(norm='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be72a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
